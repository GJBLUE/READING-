# 第二章 提供推荐  
-----

机器学习就是允许计算机不断地进行学习，对数据中重要的特征进行归纳，并借此得到一个模型。<br/>
协作型过滤：一般是对一大群人进行搜索，并从中找出品味相近的人。<br/>
<font size=5>**基于用户的协作型过滤**</font><br/>
**1. 搜集偏好**<br/>
**2. 寻找相近用户**<br/>
    - 欧几里德距离<br/>
        即一个多维度的坐标轴，将数据绘制到坐标轴上，比较其相关距离。<br/>
    - 皮尔逊相关度<br/>
        该相关系数是判断两组数据和某一直线拟合程度的一种度量。<br/>
**3. 为评论者打分**<br/>
**4. 推荐物品**<br/>
**5. 匹配商品**<br/>  
<font size=5>**基于物品的协作型过滤**</font><br/>
**总体思路**:为每件物品预先计算好最为相近的其他物品，当为某位用户提供推荐时，可以查看他曾经评过分的物品。从中挑选出排位靠前的，再构造出一个加权列表。(物品间比较不会像用户那样频繁。换个角度看待数据，求一本书的评分，可以书当key，而人们的评分当value。基于物品推荐比基于人物推荐更快。<br/>
**1. 构造物品比较数据集**<br/>
      找到数据的相似性参数，在考虑算法，比如书中举例，2维数组，求和，求方差，求方差和等等。<br/>
**2. 获得推荐**<br/>

# 第三章 获得群组  
---
**数据聚类**：寻找紧密相关的人，实物或观点，将其可视化的方法。 <br/>
**聚类算法**：目标是采集数据，然后从中找出不同的群组。<br/> 
**分类聚类**：不断的将最相似的群组两两合并，构造出一个群组的层级结构。树状图就是一种分类聚类。 <br/>
**列聚类**：对行或者列上的数据进行聚类，一般最终会转化为矩阵或者树状图。 <br/>
**K-均值聚类算法** <br/>:先随机确定K个中心位置，然后将将各个数据项分配给最临近的中心点。分配完成后，聚类中心会移动到分配给该聚类的所有节点的平均位置处。然后继续分配，直到此过程不再产生变化。<br/>
多维缩放，为数据集找到一种二维表达形式。

# 第四章 搜索与排名
---

**搜索引擎** == 爬虫 + 索引 + 检索，检索是最为奥妙的地方。<br/>
**1. 爬虫的设计**  <br/>
即找到一个搜集文档的方法。<br/>
**2. 建立索引**  <br/>
索引对应于一个列表，其中包含了所有不同的单词，这些单词所在的文档，以及单次在文档中出现的位置。<br/>
**3. 建立数据库**  <br/>
    实现索引功能<br/>
**4. 在网页中查找单词  <br/>
5. 加入索引  <br/>
6. 查询  <br/>
7. 基于内容的排名**  <br/>
    - 单词频度  <br/>
      位于擦汗寻条件中的单词在文档中出现的次数能帮助我们判断文档的相关程度。<br/>
    - 文档位置  <br/>
      文档的主题可能会出现在靠近文档的开始处。<br/>
    - 单次距离  <br/>
      如果查询条件中有多个单词，则他们在文档中出现的位置应该靠得很近。<br/>
**8. 归一化函数**  <br/>
    对于不同方法的返回结果进行比较，我们需要一种对结果进行归一化处理的办法。即，令它们具有相同的值域或变化方向。<br/>
**9. 单词频度**  <br/>
    根据查询条件中的单次在网页中出现的次数对网页进行评价。<br/>
**10. 文档位置**  <br/>
    即搜索单词在网页中的位置。<br/>
**11. 单词距离**  <br/>
  
<font size=5>**利用外部回指链接**</font>
**1. 简单计数**  <br/>
      在每个网页上统计链接数目，并将链接总数作为针对网页的度量。<br/>
**2. PageRank算法**  <br/>
      PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。<br/>
      一个页面的“得票数”由所有链向它的页面的重要性来决定，到一个页面的超链接相当于对该页投一票。一个页面的PageRank是由所有链向它的页面（“链入页面”）的重要性经过递归算法得到的。一个有较多链入的页面会有较高的等级，相反如果一个页面没有任何链入页面，那么它没有等级。<br/>
**3. 利用链接文本** <br/> 
      根据指向某一网页的链接文本来决定网页的相关度。相比于被链接的网页自身所提供的信息而言，我们从指向该网页链接中所得到的信息会更有价值。<br/>

<font size=5>**从点击中学习**</font>  
    通过追踪用户的点击链接，可以推断出用户的喜好。由此我们可以构造一个**人工神经网络**，向其提供：查询条件中的单词，返回给用户的搜索结果，以及用户的点击决策，然后对其加以训练。<br/>
**1. 构建神经网络**  <br/>
    黑盒原理，对外只有输入和输出，内部则有多个隐藏层，对输入的数据进行处理。<br/>
**2. 内部处理方法**  <br/>
    - 前馈法  <br/>
    - 反向传播法  <br/>

# 第五章 优化
---

**1. 成本函数**  <br/>
    任何优化算法的目标，就是要寻找一组能够使成本函数的返回结果达到最小化的输入。<br/>
**2. 随机搜索**  <br/>
    一般数据处于无序，随机时，适用。<br/>
**3. 爬山法**  <br/>
    随机搜索的替代办法,以一个随机解开始，然后在起临近的解集中寻找更好的题解。<br/>
**4. 模拟退火算法**  <br/>
    以一个问题的随机解开始。它用一个变量来表示温度，这一温度开始时非常高，然后逐渐降低。在每一次迭代期间，算发挥随机选中题解中的某个数字，然后朝某个方向变化。**关键在于：**如果新的成本值更低，则新的题解就会被称为当前题解。这和爬山法非常相似。不过，如果成本值更高的话，则新的题解任将可能成为当前题解。<br/>
**5. 遗传算法** <br/> 
    这类算法的运行过程是先随机生成一组解，即种群。优化过程中的每一步，算法会计算整个种群的成本函数，从而得到一个有关题解的有序函数，即新的种群。<br/>
<font size=5>**涉及偏好的优化**</font><br/>
    尽可能让最优解的成本为零。<br/>

# 第六章 文档过滤
-----

**1. 文档和单词**  <br/>
即将构造的分类器需要利用某些特征来对不同的内容项进行分类。所谓特征，是指任何可以用来判断内容中具备或者缺失的东西。当考虑对文档进行分类时，所谓的内容是文档，而特征则是文档中的单词。当将单词作为特征时，其假设为：某些单次相对而言更有可能出现于垃圾信息中。<br/>
**2. 对分类器进行训练**  <br/>
**3. 计算概率**  <br/>
比如用一个单词在一篇属于某个分类的文档中出现的次数<br/>
<font size=5>**朴素分类器**</font><br/>
在求出指定单词在一篇属于某个分类的文档中出现的概率，就需要有一种办法将各个单词的概率进行组合，从而得出整片文档属于该分类的概率。

# 第七章 决策树建模
---
**1. 引入决策树**<br/>
决策树市一种更为简单的机器学习方法，它是对被观测数据(observations)进行分类的一种相当直观的方法，决策树在经过训练之后，看起来就像是以梳妆形式排列的一系列if-then语句。<br/>
**2. 对树进行训练**<br/>
本书采用分类回归树算法(CART)，首先创建一个根结点，然后通过评估表中的所有观测变量，从中找出最合适的变量对数据进行拆分。为此，算法考差了所有不同的变量，然后从中选出一个条件对结果数据进行分解。<br/>
**3. 熵**<br/>
在信息理论中，熵代表的集合的无需程度--即集合的混乱程度。<br/>
**4. 以递归方式构造树**<br/>
为了弄明白一个属性的好坏，我们的算法首先求出整个群组的熵，然后尝试利用每个属性的可能趋之对群组进行拆分，并求出两个新群组的熵。为了确定哪个属性最适合用来拆分，算法会计算相应的信息增益。所谓信息增益，是指当前熵与两个新群组经加权平均后的熵之间的差值。算法会针对每个属性计算相应的信息增益，然后从中选出信息增益最大的属性。<br/>
**5. 决策树的剪枝**<br/>
决策树有可能会过于针对训练数据，即**过度拟合**。专门针对训练集所创建出来的分支，其熵值与真实情况相比可能会有所降低，但决策树上的判断条件实际上是完全随意的。一般采用的办法是，只要当熵减少的数量小于某个最小值时，就停止分支的创建。<br/>
整体策略为：先构造好如前所述的整棵树，然后再尝试消除多余的结点。<br/>
**6. 处理缺失数据**<br/>
如果我们缺失了某些数据，而这些数据是确定确定分支走向所必需的，那么实际上我们可以选择两个分支都走。不过此处不是平均地统计各分支对应的结果值，而是对其进行加权统计。在一棵基本的决策树中，所有的结点都隐含有一个值为1的权重，即**观测数据**对于数据项是否属于某个特定分类的概率具有100%的影响。而如果要走多个分支的话，那么我们可以给每个分支附以一个权重，其值等于所有位于该分支的其他数据行所占的必中。<br/>

# 第八章 构建价格模型
---
**1. 构建一个样本数据**<br/>
本章以葡萄酒加个举例<br/>
**2. K-最近邻算法**<br/>
找到几瓶情况最为相近的酒，并假设其价格大体相同。算法通过寻找与当前所关注的情况相似的一组商品，对这些商品的价格进行求均值，进而做出价格预测，这种办法称之为K-最近邻算法(KNN)。<br/>
**- 近邻数**<br/>
KNN中K代表的，是为了求的最终结果而参与求平均运算的商品数量。对于理想情况下的数据集，我们可以令K=1，这意味着我们只会选择距离最近的邻居，并将其加个作为最终的答案。不过实际中，我们需要故意引入“噪声”来模拟这一情况(随即加减20%)。由于有了这些噪声，就产生了误差，所以需要多选取一些近邻，然后对他们去平均值，以此来减少噪声。<br/>
**- 为近邻分配权重**<br/>
根据距离的远近为其附以相应的权重。<br/>
可采用**反函数**or**减法函数**or**高斯函数**<br/>
**- 交叉验证**<br/>
交叉验证是将数据拆分成训练集与测试集的一系列技术的统称。我们将训练集传入算法，随着正确答案的得出，我们得到了一组用以进行预测的数据集。随后，我们要求算法对测试集中的每一项数据都做出预测。其所给出的答案，将与正确答案进行对比，算法会计算出一个整体分值，以评估其所做预测的准确程度。<br/>
**3. 不同类型的变量**<br/>
**- 加入数据集**<br/>
**- 按比例缩放**<br/>
由于不是根据一种变量来计算距离，隐次我们需要对数值进行归一化处理的方法，从而使所有变量都位于相同的值域范围之内。<br/>
**- 对缩放结果进行优化**<br/>
**4. 不对称分布**<br/>
由于购买葡萄酒的途径有很多，有人有折扣，有人没，所以需要更具体的考察。<br/>
**- 估计概率密度**
**- 绘制概率分布**

# 第九章 高阶分类：核方法与SVM
---
本章数据为婚介网站用户寻找配对<br/>
**1. 构建数据集**<br/>
**2. 基本的线性分类**<br/>
线性分类的工作原理为寻找每个分类中所有数据的平均值，并构造一个代表该分类中心位置的点。然后我们就可以通过判断距离哪个中心点位置最近来对新的坐标点进行分类了。<br/>
**3. 分类特征**<br/>
**- 是否问题**<br/>
将数据转化为数值类型的一个最简单例子就是“是/否”问题了，是==1，否==-1，还可以加个不清楚==0。<br/>
**- 兴趣列表**<br/>
嗯，将兴趣爱好变为变量，并将这些想去爱好按层级排列。<br/>
**4. 构建新的数据集**<br/>
**5. 对数据进行缩放处理**<br/>
**6. 核技法**<br/>
思路是：用一个新的函数来取代原来的点积函数，当借助某个映射函数将数据第一次变换到更高维度的坐标空间时，新函数将会返回高维度坐标空间内的点积结果。但在现实中我们只会采用少数几种变换方法。其中用的最多的，被称为**径向基函数**。<br/>
径向基函数与点积类似，它接受两个向量作为输入参数，并返回一个标量值。<br/>
**7. 支持向量机**<br/>
思路是：尝试寻找一条尽可能远离所有分类的线，这条线被称为**最大间隔超平面**。

# 第十章 寻找独立特征
---
**特征提取**指尝试从数据集中寻找新的数据行，将这些新找到的数据行加以组合，就可以构建出数据集。和原始数据集不一样，位于新数据集中的每一行数据并不属于某个聚类，而是由若干特征的组合构建而成。<br/>
**1. 转换成矩阵**<br/>
将我们所获得的数据转换为矩阵。为了缩小矩阵大小，出现频率少的数据可以去掉。<br/>
**2. 分解矩阵**<br/>
对矩阵进行因式分解，得到两个更小的的矩阵，使得二者相乘以得到原来的矩阵。这两个矩阵分别是**特征矩阵**和**权重矩阵**。<br/>

# 第十一章 智能进化
---
**1. 遗传编程**<br/>
工作原理：以一大堆程序(被称为种群)开始---这些程序可以随机产生，也可以是人为设计，并且他们被认为是在某种程度上的一组优解。随后，这些程序将会在一个由用户定义的任务中展开竞争。<br/>
接下来，算法可以采取两种不同的方式：<br/>
**- 变异**：算法会对程序的某些部分以随机的方式稍作修改。<br/>
**- 配对**：先将某个最优程序的一部分去掉，然后选择其它最优程序的某一部分替代之。<br/>
**2. 遗传算法**<br/>
**- 将程序以树形式表示**<br/>
**- 树的构造和评估**<br/>
**- 程序的展现**<br/>
**3. 构造初始种群**<br/>
通常的初始种群都是由一组随即程序构成的，这样做可以使我们的起点变得更低。<br/>
创建一个随即程序的步骤如下：创建根结点并为其随机指定一个关联函数，然后再随即创建尽可能多的子节点；相应地，这些子节点也可能会有它们自己的随机关联子节点。和大多数对树进行操作的函数一样，这一过程很容易以递归的形式进行定义。<br/>
**4. 测试解题**<br/>
**- 一个简单的数学测试**<br/>
**- 衡量程序的好坏**<br/>
**5. 对程序进行变异**<br/>
- 修改节点上的函数或改变节点的分支。<br/>
- 利用一颗全新的树来替换某一子树。<br/>
**6. 交叉**<br/>
从众多程序中选出两个表现优异者，并将其组合在一起构造出一个新的程序，通常的组合方式是用一棵树的分支取代另一棵树的分枝。<br/>
**7. 多样性的重要价值**<br/>
仅仅选择表现优异的少数几个题解很快就会使种群变得极端同质化，或称为近亲交配。在这些题解之间进行的交叉操作最终会导致种群内的题解变得越来越相似，我们称之为**局部最大化**。<br/>
为了改善这一点，我们允许表现较差的题解进入最终的种群之中。<br/>

# 第十二章 算法总结
---
**1. 贝叶斯分类器**<br/>
**- 训练**<br/>
贝叶斯分类器是利用样本进行训练的，每个样本包含了一个特征列表和对应的分类。<br/>
分类器记录了它迄今为止见过的所有特征，以及这些特征与某个特定分类相关联的数字概率。分类器逐一接受样本的训练。当经过某个样本训练之后，分类器会更新该样本中特征与分类的概率，同时还会产生一个新的概率。<br/>
**- 分类**<br/>
当一个贝叶斯分类器经过训练之后，我们就可以利用它来对新的项目进行自动分类了。<br/>
**- 代码的使用说明**<br/>
我们所需要做的唯一一件事就是定义一个特征提取函数，该函数的作用是将我们用以训练或分类的数据转化成一个特征列表。<br/>
**- 优点和缺点**<br/>
**优点**：速度快，适用于大规模的训练集；对分类器实际学习状况的解释相对简单。<br/>
**缺陷**： 无法处理基于特征组合所产生的变化结果。<br/>
**2. 决策树分类器**<br/>
从根节点开始，对每个节点的判断条件进行检查--如果节点的判断条件满足，就走Yes分支，否则走No分支，一直持续到代表预测分类的那个叶节点为止。<br/>
**- 训练**<br/>
算法从根部开始构造决策树，在每一步中他都会选择一个属性，利用该属性以最佳的可能方式对数据进行拆分。<br/>
但是当面对一个大规模数据集时，就不会有清晰的拆分结果了。所以引入了熵的概念。集合中熵偏小，就意味着该集合中的大部分元素都是同质的；而熵等于0，则代表集合中的所有元素都是同一类型的。<br/>
**- 优点和缺点**<br/>
**优点**：解释一个受训模型是非常容易的，而且算法将最为重要的判断因素都很好地安排在了靠近输的根部位置。<br/>
**缺点**：不支持增量训练，因为每次训练都是重新开始，会导致分类效率降低。<br/>
**3. 神经网络**<br/>
神经网络可以识别出哪些单词的组合是重要的，以及哪些单词对于某次查询是不重要的。神经网络不仅可以用于分类，还可以用于数值预测问题。<br/>
本书涉及的神经网络被称为多层感知器网络，它包含一层输入神经元，这些神经元会将输入传递给一层或多层隐藏神经元。层与层之间通过**突触**彼此相连，每个突触都有一个与之关联的权重。一组神经元的输出是通过突触传入下一层的。如果从一个神经元指向下一个神经元的突触权重越大，那么它对神经元输出的影响也就越大。<br/>
**- 训练神经网络**<br/>
反向传播法<br/>
**- 优点和缺点**<br/>
**优点**：他们能够处理复杂的非线性函数，并能发现不同输入间的依赖关系，同时允许增量训练。<br/>
**缺点**：由于是黑盒方法，无法推导过程。在选择训练数据的比率及与问题相适应的网络规模方面，并没有明确的规则可以遵循。<br/>
**4. 支持向量机**<br/>
SVM通过寻找介于两个分类间的分界线来构建预测模型。确定这条分界线所在位置唯一需要的坐标点，是距离它最近的那些点，这些点被称为支持向量。<br/>
**- 核技法**<br/>
**- 优点和缺点**<br/>
**优点**：对观测数据进行分类时速度极快。<br/>
**缺点**：针对每个数据集的最佳核变换函数及其相应的参数都是不一样的，而且每当遇到新的数据集时都必须重新确定这些函数及其参数。<br/>
**5. K-最近邻**<br/>
KNN接受一个用以进行数值预测的新数据项，然后将
其与一组已经赋过值的数据项进行比较。算法会从中找出与待预测数据项最为接近的若干项，并对其求均值以得到最终的预测结果。<br/>
**- 变量缩放及多余变量**<br/>
我们给出了一种数据调整的方法，该方法对某些变量的数值进行了放大，对其他变量则做了缩小。因为对数据的缩放量取决于具体的应用，所以可以通过对预测算法实施交叉验证，来判断一组缩放因子的优劣程度。<br/>
**- 优点和缺点**<br/>
**优点**：可以利用复杂函数进行数值预测，同时保持简单易懂的特点。<br/>
**缺点**：为了完成预测，它要求所有的训练数据都缺一不可。<br/>
** 6. 聚类**<br/>
分级聚类和K-均值聚类都属于非监督学习技术。<br/>
**- 分级聚类**<br/>
分级聚类寻找两个距离最接近的数据项，然后将它们合二为一，最后形成了一种树的形式。<br/>
**- K-均值聚类**<br/>
K-均值聚类实际上是将数据拆分到不同的种群中，要求我们在开始执行算法之前给出想要的群组数量。<br/>
**7. 多维缩放**<br/>
多维缩放也是一种非监督技术，它会为数据集构造一个低维度的表达形式，并令距离值尽可能接近原始数据集。<br/>
**8. 非负矩阵因式分解**<br/>
非负矩阵因式分解将一组数值型的观测拆解成不同的组分。<br/>
**9. 优化**<br/>
优化是要尝试找到能够使成本函数的输出结果达到最小化的值。<br/>
**- 成本函数**<br/>
成本函数接受一个经推测得到的题解，并返回一个数值结果，该值越大就表示题解的表现越差，该值越小就表示题解的表现越好。<br/>
**10. 模拟退火**<br/>
以一个随机推测的题解开始，然后以此为基准随机选择一个方向，并就近找到另一个相似解。算法希望借此来改善题解的表现，如果题解成本变小，则新题解将取代原来的题解。<br/>
**11. 遗传算法**<br/>
以一组被称为种群被称为种群的随机题解开始。种群中表现最为优异的成员--即成本最低者--会被选中并通过稍事改变(变异)或特征组合(即变叉或配对)的方式加以修改。随后，我们会得到一个新种群，如此循环。<br/>