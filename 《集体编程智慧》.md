# 第二章 提供推荐  
-----

机器学习就是允许计算机不断地进行学习，对数据中重要的特征进行归纳，并借此得到一个模型。<br/>
协作型过滤：一般是对一大群人进行搜索，并从中找出品味相近的人。<br/>
<font size=5>**基于用户的协作型过滤**</font><br/>
**1. 搜集偏好**<br/>
**2. 寻找相近用户**<br/>
    - 欧几里德距离<br/>
        即一个多维度的坐标轴，将数据绘制到坐标轴上，比较其相关距离。<br/>
    - 皮尔逊相关度<br/>
        该相关系数是判断两组数据和某一直线拟合程度的一种度量。<br/>
**3. 为评论者打分**<br/>
**4. 推荐物品**<br/>
**5. 匹配商品**<br/>  
<font size=5>**基于物品的协作型过滤**</font><br/>
**总体思路**:为每件物品预先计算好最为相近的其他物品，当为某位用户提供推荐时，可以查看他曾经评过分的物品。从中挑选出排位靠前的，再构造出一个加权列表。(物品间比较不会像用户那样频繁。换个角度看待数据，求一本书的评分，可以书当key，而人们的评分当value。基于物品推荐比基于人物推荐更快。<br/>
**1. 构造物品比较数据集**<br/>
      找到数据的相似性参数，在考虑算法，比如书中举例，2维数组，求和，求方差，求方差和等等。<br/>
**2. 获得推荐**<br/>

# 第三章 获得群组  
---

**数据聚类**：寻找紧密相关的人，实物或观点，将其可视化的方法。 <br/>
**聚类算法**：目标是采集数据，然后从中找出不同的群组。<br/> 
**分类聚类**：不断的将最相似的群组两两合并，构造出一个群组的层级结构。树状图就是一种分类聚类。 <br/>
**列聚类**：对行或者列上的数据进行聚类，一般最终会转化为矩阵或者树状图。 <br/>
**K-均值聚类算法** <br/>:先随机确定K个中心位置，然后将将各个数据项分配给最临近的中心点。分配完成后，聚类中心会移动到分配给该聚类的所有节点的平均位置处。然后继续分配，直到此过程不再产生变化。<br/>
多维缩放，为数据集找到一种二维表达形式。

# 第四章 搜索与排名
---

**搜索引擎** == 爬虫 + 索引 + 检索，检索是最为奥妙的地方。<br/>
**1. 爬虫的设计**  <br/>
即找到一个搜集文档的方法。<br/>
**2. 建立索引**  <br/>
索引对应于一个列表，其中包含了所有不同的单词，这些单词所在的文档，以及单次在文档中出现的位置。<br/>
**3. 建立数据库**  <br/>
    实现索引功能<br/>
**4. 在网页中查找单词  <br/>
5. 加入索引  <br/>
6. 查询  <br/>
7. 基于内容的排名**  <br/>
    - 单词频度  <br/>
      位于擦汗寻条件中的单词在文档中出现的次数能帮助我们判断文档的相关程度。<br/>
    - 文档位置  <br/>
      文档的主题可能会出现在靠近文档的开始处。<br/>
    - 单次距离  <br/>
      如果查询条件中有多个单词，则他们在文档中出现的位置应该靠得很近。<br/>
**8. 归一化函数**  <br/>
    对于不同方法的返回结果进行比较，我们需要一种对结果进行归一化处理的办法。即，令它们具有相同的值域或变化方向。<br/>
**9. 单词频度**  <br/>
    根据查询条件中的单次在网页中出现的次数对网页进行评价。<br/>
**10. 文档位置**  <br/>
    即搜索单词在网页中的位置。<br/>
**11. 单词距离**  <br/>
  
<font size=5>**利用外部回指链接**</font>
**1. 简单计数**  <br/>
      在每个网页上统计链接数目，并将链接总数作为针对网页的度量。<br/>
**2. PageRank算法**  <br/>
      PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。Google把从A页面到B页面的链接解释为A页面给B页面投票，Google根据投票来源（甚至来源的来源，即链接到A页面的页面）和投票目标的等级来决定新的等级。简单的说，一个高等级的页面可以使其他低等级页面的等级提升。<br/>
      一个页面的“得票数”由所有链向它的页面的重要性来决定，到一个页面的超链接相当于对该页投一票。一个页面的PageRank是由所有链向它的页面（“链入页面”）的重要性经过递归算法得到的。一个有较多链入的页面会有较高的等级，相反如果一个页面没有任何链入页面，那么它没有等级。<br/>
**3. 利用链接文本** <br/> 
      根据指向某一网页的链接文本来决定网页的相关度。相比于被链接的网页自身所提供的信息而言，我们从指向该网页链接中所得到的信息会更有价值。<br/>

<font size=5>**从点击中学习**</font>  
    通过追踪用户的点击链接，可以推断出用户的喜好。由此我们可以构造一个**人工神经网络**，向其提供：查询条件中的单词，返回给用户的搜索结果，以及用户的点击决策，然后对其加以训练。<br/>
**1. 构建神经网络**  <br/>
    黑盒原理，对外只有输入和输出，内部则有多个隐藏层，对输入的数据进行处理。<br/>
**2. 内部处理方法**  <br/>
    - 前馈法  <br/>
    - 反向传播法  <br/>

# 第五章 优化
---

**1. 成本函数**  <br/>
    任何优化算法的目标，就是要寻找一组能够使成本函数的返回结果达到最小化的输入。<br/>
**2. 随机搜索**  <br/>
    一般数据处于无序，随机时，适用。<br/>
**3. 爬山法**  <br/>
    随机搜索的替代办法,以一个随机解开始，然后在起临近的解集中寻找更好的题解。<br/>
**4. 模拟退火算法**  <br/>
    以一个问题的随机解开始。它用一个变量来表示温度，这一温度开始时非常高，然后逐渐降低。在每一次迭代期间，算发挥随机选中题解中的某个数字，然后朝某个方向变化。**关键在于：**如果新的成本值更低，则新的题解就会被称为当前题解。这和爬山法非常相似。不过，如果成本值更高的话，则新的题解任将可能成为当前题解。<br/>
**5. 遗传算法** <br/> 
    这类算法的运行过程是先随机生成一组解，即种群。优化过程中的每一步，算法会计算整个种群的成本函数，从而得到一个有关题解的有序函数，即新的种群。<br/>
<font size=5>**涉及偏好的优化**</font><br/>
    尽可能让最优解的成本为零。<br/>

# 第六章 文档过滤
-----

**1. 文档和单词**  <br/>
即将构造的分类器需要利用某些特征来对不同的内容项进行分类。所谓特征，是指任何可以用来判断内容中具备或者缺失的东西。当考虑对文档进行分类时，所谓的内容是文档，而特征则是文档中的单词。当将单词作为特征时，其假设为：某些单次相对而言更有可能出现于垃圾信息中。<br/>
**2. 对分类器进行训练**  <br/>
**3. 计算概率**  <br/>
比如用一个单词在一篇属于某个分类的文档中出现的次数<br/>
<font size=5>**朴素分类器**</font><br/>
在求出指定单词在一篇属于某个分类的文档中出现的概率，就需要有一种办法将各个单词的概率进行组合，从而得出整片文档属于该分类的概率。

# 第七章 决策树建模
---
**1. 引入决策树**<br/>
决策树市一种更为简单的机器学习方法，它是对被观测数据(observations)进行分类的一种相当直观的方法，决策树在经过训练之后，看起来就像是以梳妆形式排列的一系列if-then语句。<br/>
**2. 对树进行训练**<br/>
本书采用分类回归树算法(CART)，首先创建一个根结点，然后通过评估表中的所有观测变量，从中找出最合适的变量对数据进行拆分。为此，算法考差了所有不同的变量，然后从中选出一个条件对结果数据进行分解。<br/>
**3. 熵**<br/>
在信息理论中，熵代表的集合的无需程度--即集合的混乱程度。<br/>
**4. 以递归方式构造树**<br/>
为了弄明白一个属性的好坏，我们的算法首先求出整个群组的熵，然后尝试利用每个属性的可能趋之对群组进行拆分，并求出两个新群组的熵。为了确定哪个属性最适合用来拆分，算法会计算相应的信息增益。所谓信息增益，是指当前熵与两个新群组经加权平均后的熵之间的差值。算法会针对每个属性计算相应的信息增益，然后从中选出信息增益最大的属性。<br/>
**5. 决策树的剪枝**<br/>
决策树有可能会过于针对训练数据，即**过度拟合**。专门针对训练集所创建出来的分支，其熵值与真实情况相比可能会有所降低，但决策树上的判断条件实际上是完全随意的。一般采用的办法是，只要当熵减少的数量小于某个最小值时，就停止分支的创建。<br/>
整体策略为：先构造好如前所述的整棵树，然后再尝试消除多余的结点。<br/>
**6. 处理缺失数据**<br/>
如果我们缺失了某些数据，而这些数据是确定确定分支走向所必需的，那么实际上我们可以选择两个分支都走。不过此处不是平均地统计各分支对应的结果值，而是对其进行加权统计。在一棵基本的决策树中，所有的结点都隐含有一个值为1的权重，即**观测数据**对于数据项是否属于某个特定分类的概率具有100%的影响。而如果要走多个分支的话，那么我们可以给每个分支附以一个权重，其值等于所有位于该分支的其他数据行所占的必中。<br/>

# 第八章 构建价格模型
---
**1. 构建一个样本数据**<br/>
本章以葡萄酒加个举例<br/>
**2. K-最近邻算法**<br/>
找到几瓶情况最为相近的酒，并假设其价格大体相同。算法通过寻找与当前所关注的情况相似的一组商品，对这些商品的价格进行求均值，进而做出价格预测，这种办法称之为K-最近邻算法(KNN)。<br/>
**- 近邻数**<br/>
KNN中K代表的，是为了求的最终结果而参与求平均运算的商品数量。对于理想情况下的数据集，我们可以令K=1，这意味着我们只会选择距离最近的邻居，并将其加个作为最终的答案。不过实际中，我们需要故意引入“噪声”来模拟这一情况(随即加减20%)。由于有了这些噪声，就产生了误差，所以需要多选取一些近邻，然后对他们去平均值，以此来减少噪声。<br/>
**- 为近邻分配权重**<br/>
根据距离的远近为其附以相应的权重。<br/>
可采用**反函数**or**减法函数**or**高斯函数**<br/>
**- 交叉验证**<br/>
交叉验证是将数据拆分成训练集与测试集的一系列技术的统称。我们将训练集传入算法，随着正确答案的得出，我们得到了一组用以进行预测的数据集。随后，我们要求算法对测试集中的每一项数据都做出预测。其所给出的答案，将与正确答案进行对比，算法会计算出一个整体分值，以评估其所做预测的准确程度。<br/>
**3. 不同类型的变量**<br/>
**- 加入数据集**<br/>
**- 按比例缩放**<br/>
由于不是根据一种变量来计算距离，隐次我们需要对数值进行归一化处理的方法，从而使所有变量都位于相同的值域范围之内。<br/>
**- 对缩放结果进行优化**<br/>
**4. 不对称分布**<br/>
由于购买葡萄酒的途径有很多，有人有折扣，有人没，所以需要更具体的考察。<br/>
**- 估计概率密度**
**- 绘制概率分布**

# 第九章 高阶分类：核方法与SVM
---
本章数据为婚介网站用户寻找配对<br/>
**1. 构建数据集**<br/>
**2. 基本的线性分类**<br/>
线性分类的工作原理为寻找每个分类中所有数据的平均值，并构造一个代表该分类中心位置的点。然后我们就可以通过判断距离哪个中心点位置最近来对新的坐标点进行分类了。<br/>
**3. 分类特征**<br/>
**- 是否问题**<br/>
将数据转化为数值类型的一个最简单例子就是“是/否”问题了，是==1，否==-1，还可以加个不清楚==0。<br/>
**- 兴趣列表**<br/>
嗯，将兴趣爱好变为变量，并将这些想去爱好按层级排列。<br/>
**4. 构建新的数据集**<br/>
**5. 对数据进行缩放处理**<br/>
**6. 核技法**<br/>
思路是：用一个新的函数来取代原来的点积函数，当借助某个映射函数将数据第一次变换到更高维度的坐标空间时，新函数将会返回高维度坐标空间内的点积结果。但在现实中我们只会采用少数几种变换方法。其中用的最多的，被称为**径向基函数**。<br/>
径向基函数与点积类似，它接受两个向量作为输入参数，并返回一个标量值。<br/>
**7. 支持向量机**<br/>
思路是：尝试寻找一条尽可能远离所有分类的线，这条线被称为**最大间隔超平面**。

# 第十章 寻找独立特征
---
**特征提取**指尝试从数据集中寻找新的数据行，将这些新找到的数据行加以组合，就可以构建出数据集。和原始数据集不一样，位于新数据集中的每一行数据并不属于某个聚类，而是由若干特征的组合构建而成。<br/>
**1. 转换成矩阵**<br/>
将我们所获得的数据转换为矩阵。为了缩小矩阵大小，出现频率少的数据可以去掉。<br/>
**2. 分解矩阵**<br/>
对矩阵进行因式分解，得到两个更小的的矩阵，使得二者相乘以得到原来的矩阵。这两个矩阵分别是**特征矩阵**和**权重矩阵**。<br/>

# 第十一章 智能进化
---
**1. 遗传编程**<br/>
工作原理：以一大堆程序(被称为种群)开始---这些程序可以随机产生，也可以是人为设计，并且他们被认为是在某种程度上的一组优解。随后，这些程序将会在一个由用户定义的任务中展开竞争。<br/>
接下来，算法可以采取两种不同的方式：<br/>
**- 变异**：算法会对程序的某些部分以随机的方式稍作修改。<br/>
**- 配对**：先将某个最优程序的一部分去掉，然后选择其它最优程序的某一部分替代之。<br/>